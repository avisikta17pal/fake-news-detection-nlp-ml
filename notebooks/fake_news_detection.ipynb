{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b6d7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# üö® Fake News Detection using NLP and Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Project Overview:** This notebook demonstrates a complete pipeline for detecting fake news articles using Natural Language Processing (NLP) and Machine Learning techniques. The model uses TF-IDF vectorization combined with advanced features including sentiment analysis, readability scores, and linguistic features to classify news articles as either real or fake.\\n\",\n",
    "    \"\\n\",\n",
    "    \"**Phase 1 Improvements:**\\n\",\n",
    "    \"- ‚úÖ Replaced stemming with lemmatization using spaCy\\n\",\n",
    "    \"- ‚úÖ Preserved named entities and used POS tagging to filter tokens\\n\",\n",
    "    \"- ‚úÖ Added sentiment polarity scores using TextBlob and VADER\\n\",\n",
    "    \"- ‚úÖ Added readability scores (Flesch Reading Ease and others)\\n\",\n",
    "    \"- ‚úÖ Enabled toggle in preprocessing to turn these additional features on or off\\n\",\n",
    "    \"- ‚úÖ Updated model training to include these new features alongside TF-IDF vectors\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"## üìã Table of Contents\\n\",\n",
    "    \"1. [Setup and Imports](#setup)\\n\",\n",
    "    \"2. [Data Loading and Exploration](#data-loading)\\n\",\n",
    "    \"3. [Advanced Data Preprocessing](#preprocessing)\\n\",\n",
    "    \"4. [Exploratory Data Analysis](#eda)\\n\",\n",
    "    \"5. [Feature Engineering](#feature-engineering)\\n\",\n",
    "    \"6. [Model Training](#model-training)\\n\",\n",
    "    \"7. [Model Evaluation](#evaluation)\\n\",\n",
    "    \"8. [Sample Predictions](#predictions)\\n\",\n",
    "    \"9. [Conclusions](#conclusions)\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Setup and Imports <a name=\\\"setup\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, let's import all the necessary libraries and our custom modules.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Import standard libraries\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import scikit-learn components\\n\",\n",
    "    \"from sklearn.model_selection import train_test_split\\n\",\n",
    "    \"from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import our custom modules\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"sys.path.append('../src')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from data_loader import load_data\\n\",\n",
    "    \"from preprocessing import prepare_data, clean_text, get_text_statistics, extract_text_features\\n\",\n",
    "    \"from visualization import generate_wordcloud, plot_label_distribution, plot_article_length_distribution\\n\",\n",
    "    \"from model import train_model, load_model, predict_text, get_feature_importance, evaluate_model_performance\\n\",\n",
    "    \"from evaluation import evaluate_model, generate_evaluation_report\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set plotting style\\n\",\n",
    "    \"plt.style.use('seaborn-v0_8')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"plt.rcParams['figure.figsize'] = (12, 8)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"‚úÖ All libraries imported successfully!\\\")\\n\",\n",
    "    \"print(\\\"üîß Phase 1 improvements enabled:\\\")\\n\",\n",
    "    \"print(\\\"   - spaCy lemmatization\\\")\\n\",\n",
    "    \"print(\\\"   - Named entity preservation\\\")\\n\",\n",
    "    \"print(\\\"   - POS tagging and filtering\\\")\\n\",\n",
    "    \"print(\\\"   - Sentiment analysis (TextBlob + VADER)\\\")\\n\",\n",
    "    \"print(\\\"   - Readability scores\\\")\\n\",\n",
    "    \"print(\\\"   - Advanced feature extraction\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Data Loading and Exploration <a name=\\\"data-loading\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's load the dataset and explore its structure.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load the dataset\\n\",\n",
    "    \"print(\\\"üìä Loading dataset...\\\")\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    df = load_data('../data/train.csv')\\n\",\n",
    "    \"    print(f\\\"‚úÖ Dataset loaded successfully! Shape: {df.shape}\\\")\\n\",\n",
    "    \"except FileNotFoundError:\\n\",\n",
    "    \"    print(\\\"‚ùå Dataset not found! Please ensure 'train.csv' is in the data/ directory.\\\")\\n\",\n",
    "    \"    print(\\\"\\\\nüìù Note: You can download the dataset from Kaggle:\\\")\\n\",\n",
    "    \"    print(\\\"   https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Create sample data for demonstration\\n\",\n",
    "    \"    print(\\\"\\\\nüîÑ Creating sample data for demonstration...\\\")\\n\",\n",
    "    \"    sample_data = {\\n\",\n",
    "    \"        'text': [\\n\",\n",
    "    \"            \\\"Scientists discover new species of deep-sea creatures in the Pacific Ocean. The research team used advanced underwater drones to explore depths previously inaccessible to humans.\\\",\\n\",\n",
    "    \"            \\\"BREAKING: Aliens contact Earth government! Secret meeting held at Area 51. Sources say they want to share advanced technology in exchange for our natural resources.\\\",\\n\",\n",
    "    \"            \\\"New study shows that regular exercise can reduce the risk of heart disease by up to 30%. The research involved over 10,000 participants across multiple countries.\\\",\\n\",\n",
    "    \"            \\\"SHOCKING: Celebrities are actually robots controlled by the government! Insider reveals all the secrets they don't want you to know.\\\",\\n\",\n",
    "    \"            \\\"Climate change report indicates global temperatures have risen by 1.1¬∞C since pre-industrial levels. Scientists warn of severe consequences if action is not taken.\\\",\\n\",\n",
    "    \"            \\\"CONSPIRACY: The moon landing was filmed in Hollywood! NASA admits to staging the entire event to win the space race.\\\"\\n\",\n",
    "    \"        ],\\n\",\n",
    "    \"        'label': [0, 1, 0, 1, 0, 1]  # 0=Real, 1=Fake\\n\",\n",
    "    \"    }\\n\",\n",
    "    \"    df = pd.DataFrame(sample_data)\\n\",\n",
    "    \"    print(f\\\"‚úÖ Sample dataset created! Shape: {df.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Display basic information about the dataset\\n\",\n",
    "    \"print(\\\"üìã DATASET INFORMATION:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Shape: {df.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Columns: {list(df.columns)}\\\")\\n\",\n",
    "    \"print(f\\\"\\\\nData types:\\\")\\n\",\n",
    "    \"print(df.dtypes)\\n\",\n",
    "    \"print(f\\\"\\\\nMissing values:\\\")\\n\",\n",
    "    \"print(df.isnull().sum())\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display first few rows\\n\",\n",
    "    \"print(f\\\"\\\\nüìÑ First 3 rows:\\\")\\n\",\n",
    "    \"print(df.head(3))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display label distribution\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Label distribution:\\\")\\n\",\n",
    "    \"label_counts = df['label'].value_counts()\\n\",\n",
    "    \"print(label_counts)\\n\",\n",
    "    \"print(f\\\"\\\\nPercentage distribution:\\\")\\n\",\n",
    "    \"print(label_counts / len(df) * 100)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Advanced Data Preprocessing <a name=\\\"preprocessing\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now let's clean and preprocess the text data with our enhanced Phase 1 features.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Configuration for preprocessing\\n\",\n",
    "    \"USE_ADVANCED_FEATURES = True  # Toggle for advanced features\\n\",\n",
    "    \"PRESERVE_ENTITIES = True       # Toggle for named entity preservation\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"üßπ ADVANCED PREPROCESSING:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Using advanced features: {USE_ADVANCED_FEATURES}\\\")\\n\",\n",
    "    \"print(f\\\"Preserving entities: {PRESERVE_ENTITIES}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare the data with advanced features\\n\",\n",
    "    \"X, y, feature_df = prepare_data(df, use_advanced_features=USE_ADVANCED_FEATURES, preserve_entities=PRESERVE_ENTITIES)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Preprocessing completed!\\\")\\n\",\n",
    "    \"print(f\\\"Text features shape: {X.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Labels shape: {y.shape}\\\")\\n\",\n",
    "    \"if feature_df is not None:\\n\",\n",
    "    \"    print(f\\\"Additional features shape: {feature_df.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"Additional features: {list(feature_df.columns)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Get text statistics\\n\",\n",
    "    \"print(\\\"üìä TEXT STATISTICS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"stats = get_text_statistics(df)\\n\",\n",
    "    \"for key, value in stats.items():\\n\",\n",
    "    \"    print(f\\\"{key}: {value:.0f}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show some examples of cleaned text\\n\",\n",
    "    \"print(f\\\"\\\\nüìù SAMPLE CLEANED TEXTS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"for i, (original, cleaned) in enumerate(zip(df['text'].head(3), X.head(3))):\\n\",\n",
    "    \"    print(f\\\"\\\\nExample {i+1}:\\\")\\n\",\n",
    "    \"    print(f\\\"Original: {original[:100]}...\\\")\\n\",\n",
    "    \"    print(f\\\"Cleaned:  {cleaned[:100]}...\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Display advanced features if available\\n\",\n",
    "    \"if feature_df is not None:\\n\",\n",
    "    \"    print(f\\\"\\\\nüîç ADVANCED FEATURES SAMPLE:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 50)\\n\",\n",
    "    \"    print(feature_df.head(3))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show feature statistics\\n\",\n",
    "    \"    print(f\\\"\\\\nüìà FEATURE STATISTICS:\\\")\\n\",\n",
    "    \"    print(\\\"=\\\" * 50)\\n\",\n",
    "    \"    print(feature_df.describe())\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Exploratory Data Analysis <a name=\\\"eda\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's explore the data through various visualizations.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Generate visualizations\\n\",\n",
    "    \"print(\\\"üìà EXPLORATORY DATA ANALYSIS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 1. Label distribution\\n\",\n",
    "    \"plot_label_distribution(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 2. Article length distribution\\n\",\n",
    "    \"plot_article_length_distribution(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# 3. Word clouds\\n\",\n",
    "    \"print(\\\"\\\\n‚òÅÔ∏è Generating word clouds...\\\")\\n\",\n",
    "    \"generate_wordcloud(df)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\n‚úÖ All visualizations completed!\\\")\\n\",\n",
    "    \"print(\\\"üìÅ Check the 'outputs/' directory for saved plots.\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Feature Engineering <a name=\\\"feature-engineering\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"The TF-IDF vectorization and advanced features will be handled by our enhanced model training function. Let's prepare the train-test split.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Split the data into training and testing sets\\n\",\n",
    "    \"print(\\\"üîÄ SPLITTING DATA:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"X_train, X_test, y_train, y_test = train_test_split(\\n\",\n",
    "    \"    X, y, test_size=0.2, random_state=42, stratify=y\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split additional features if available\\n\",\n",
    "    \"feature_df_train = None\\n\",\n",
    "    \"feature_df_test = None\\n\",\n",
    "    \"if feature_df is not None:\\n\",\n",
    "    \"    feature_df_train = feature_df.iloc[X_train.index]\\n\",\n",
    "    \"    feature_df_test = feature_df.iloc[X_test.index]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training set size: {len(X_train)}\\\")\\n\",\n",
    "    \"print(f\\\"Test set size: {len(X_test)}\\\")\\n\",\n",
    "    \"print(f\\\"Training set label distribution:\\\")\\n\",\n",
    "    \"print(y_train.value_counts())\\n\",\n",
    "    \"print(f\\\"\\\\nTest set label distribution:\\\")\\n\",\n",
    "    \"print(y_test.value_counts())\\n\",\n",
    "    \"\\n\",\n",
    "    \"if feature_df_train is not None:\\n\",\n",
    "    \"    print(f\\\"\\\\nTraining additional features shape: {feature_df_train.shape}\\\")\\n\",\n",
    "    \"    print(f\\\"Test additional features shape: {feature_df_test.shape}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Model Training <a name=\\\"model-training\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Now let's train our enhanced model with TF-IDF vectorization and advanced features.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train the model with advanced features\\n\",\n",
    "    \"print(\\\"ü§ñ TRAINING ENHANCED MODEL:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Choose model type\\n\",\n",
    "    \"MODEL_TYPE = 'logistic'  # Options: 'logistic', 'random_forest'\\n\",\n",
    "    \"\\n\",\n",
    "    \"model, vectorizer, scaler = train_model(\\n\",\n",
    "    \"    X_train, y_train, \\n\",\n",
    "    \"    feature_df=feature_df_train,\\n\",\n",
    "    \"    model_type=MODEL_TYPE\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Model training completed!\\\")\\n\",\n",
    "    \"print(f\\\"Model type: {MODEL_TYPE}\\\")\\n\",\n",
    "    \"print(f\\\"Using advanced features: {feature_df_train is not None}\\\")\\n\",\n",
    "    \"print(f\\\"Model saved to: models/fake_news_model_{MODEL_TYPE}.pkl\\\")\\n\",\n",
    "    \"print(f\\\"Vectorizer saved to: models/tfidf_vectorizer.pkl\\\")\\n\",\n",
    "    \"if scaler:\\n\",\n",
    "    \"    print(f\\\"Feature scaler saved to: models/feature_scaler.pkl\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Get feature importance\\n\",\n",
    "    \"print(\\\"üîç FEATURE IMPORTANCE:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"top_features = get_feature_importance(\\n\",\n",
    "    \"    model, vectorizer, scaler, feature_df_train, top_n=20\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nTop 20 most important features:\\\")\\n\",\n",
    "    \"for i, (feature, importance) in enumerate(top_features, 1):\\n\",\n",
    "    \"    sentiment = \\\"üü¢ (Real)\\\" if importance < 0 else \\\"üî¥ (Fake)\\\"\\n\",\n",
    "    \"    print(f\\\"{i:2d}. {feature:25s}: {importance:8.4f} {sentiment}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Analyze feature types\\n\",\n",
    "    \"tfidf_features = [f for f, _ in top_features if not f.startswith(('textblob_', 'vader_', 'flesch_', 'pos_', 'entity_', 'avg_'))]\\n\",\n",
    "    \"sentiment_features = [f for f, _ in top_features if f.startswith(('textblob_', 'vader_'))]\\n\",\n",
    "    \"readability_features = [f for f, _ in top_features if f.startswith(('flesch_', 'gunning_', 'smog_', 'automated_', 'coleman_', 'linsear_', 'dale_'))]\\n\",\n",
    "    \"linguistic_features = [f for f, _ in top_features if f.startswith(('pos_', 'entity_', 'avg_'))]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nüìä Feature Type Analysis:\\\")\\n\",\n",
    "    \"print(f\\\"TF-IDF features: {len(tfidf_features)}\\\")\\n\",\n",
    "    \"print(f\\\"Sentiment features: {len(sentiment_features)}\\\")\\n\",\n",
    "    \"print(f\\\"Readability features: {len(readability_features)}\\\")\\n\",\n",
    "    \"print(f\\\"Linguistic features: {len(linguistic_features)}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Model Evaluation <a name=\\\"evaluation\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's evaluate our enhanced model's performance comprehensively.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate the model\\n\",\n",
    "    \"print(\\\"üìä MODEL EVALUATION:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"results = evaluate_model_performance(\\n\",\n",
    "    \"    model, vectorizer, X_test, y_test, \\n\",\n",
    "    \"    scaler=scaler, feature_df_test=feature_df_test\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Generate evaluation report\\n\",\n",
    "    \"generate_evaluation_report(results)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\n‚úÖ Evaluation completed!\\\")\\n\",\n",
    "    \"print(f\\\"üìÅ Check the 'outputs/' directory for evaluation plots and report.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Compare with baseline (TF-IDF only)\\n\",\n",
    "    \"print(f\\\"\\\\nüîç COMPARISON WITH BASELINE:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(f\\\"Enhanced model (with advanced features): {results['accuracy']:.4f}\\\")\\n\",\n",
    "    \"print(f\\\"Features used: TF-IDF + {len(feature_df.columns) if feature_df is not None else 0} additional features\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Sample Predictions <a name=\\\"predictions\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's test our enhanced model with some sample news articles.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Test predictions on sample texts\\n\",\n",
    "    \"print(\\\"üß™ SAMPLE PREDICTIONS:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"\\n\",\n",
    "    \"sample_texts = [\\n\",\n",
    "    \"    \\\"Scientists discover new species of deep-sea creatures in the Pacific Ocean. The research team used advanced underwater drones to explore depths previously inaccessible to humans.\\\",\\n\",\n",
    "    \"    \\\"BREAKING: Aliens contact Earth government! Secret meeting held at Area 51. Sources say they want to share advanced technology in exchange for our natural resources.\\\",\\n\",\n",
    "    \"    \\\"New study shows that regular exercise can reduce the risk of heart disease by up to 30%. The research involved over 10,000 participants across multiple countries.\\\",\\n\",\n",
    "    \"    \\\"SHOCKING: Celebrities are actually robots controlled by the government! Insider reveals all the secrets they don't want you to know.\\\",\\n\",\n",
    "    \"    \\\"Climate change report indicates global temperatures have risen by 1.1¬∞C since pre-industrial levels. Scientists warn of severe consequences if action is not taken.\\\"\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nPredictions for sample texts:\\\")\\n\",\n",
    "    \"for i, text in enumerate(sample_texts, 1):\\n\",\n",
    "    \"    # Extract features for the text if using advanced features\\n\",\n",
    "    \"    text_features = None\\n\",\n",
    "    \"    if feature_df is not None:\\n\",\n",
    "    \"        text_features = extract_text_features(text, use_advanced_features=True)\\n\",\n",
    "    \"        text_features_df = pd.DataFrame([text_features])\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        text_features_df = None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    prediction, probability = predict_text(\\n\",\n",
    "    \"        text, model, vectorizer, scaler, text_features_df\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    result = \\\"üî¥ FAKE\\\" if prediction == 1 else \\\"üü¢ REAL\\\"\\n\",\n",
    "    \"    confidence = max(probability) * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n{i}. {result} (Confidence: {confidence:.1f}%)\\\")\\n\",\n",
    "    \"    print(f\\\"   Text: {text[:80]}...\\\")\\n\",\n",
    "    \"    print(f\\\"   Probabilities: Real={probability[0]:.3f}, Fake={probability[1]:.3f}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Show extracted features if available\\n\",\n",
    "    \"    if text_features:\\n\",\n",
    "    \"        print(f\\\"   Sentiment (TextBlob): {text_features.get('textblob_polarity', 0):.3f}\\\")\\n\",\n",
    "    \"        print(f\\\"   Readability (Flesch): {text_features.get('flesch_reading_ease', 0):.1f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 9. Interactive Prediction <a name=\\\"interactive\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create an interactive function to test your own news articles.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Interactive prediction function\\n\",\n",
    "    \"def predict_news_article(text):\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    Predict whether a given news article is fake or real news.\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Args:\\n\",\n",
    "    \"        text (str): The news article text\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    Returns:\\n\",\n",
    "    \"        tuple: (prediction, confidence, probabilities, features)\\n\",\n",
    "    \"    \\\"\\\"\\\"\\n\",\n",
    "    \"    # Extract features for the text if using advanced features\\n\",\n",
    "    \"    text_features = None\\n\",\n",
    "    \"    if feature_df is not None:\\n\",\n",
    "    \"        text_features = extract_text_features(text, use_advanced_features=True)\\n\",\n",
    "    \"        text_features_df = pd.DataFrame([text_features])\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        text_features_df = None\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    prediction, probability = predict_text(\\n\",\n",
    "    \"        text, model, vectorizer, scaler, text_features_df\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    confidence = max(probability) * 100\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    return prediction, confidence, probability, text_features\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test with user input\\n\",\n",
    "    \"print(\\\"üéØ INTERACTIVE PREDICTION:\\\")\\n\",\n",
    "    \"print(\\\"=\\\" * 50)\\n\",\n",
    "    \"print(\\\"\\\\nEnter a news article text to classify (or press Enter to skip):\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# You can uncomment the following lines to enable interactive input\\n\",\n",
    "    \"# user_text = input(\\\"\\\\nNews article: \\\")\\n\",\n",
    "    \"# if user_text.strip():\\n\",\n",
    "    \"#     pred, conf, probs, features = predict_news_article(user_text)\\n\",\n",
    "    \"#     result = \\\"üî¥ FAKE\\\" if pred == 1 else \\\"üü¢ REAL\\\"\\n\",\n",
    "    \"#     print(f\\\"\\\\nPrediction: {result}\\\")\\n\",\n",
    "    \"#     print(f\\\"Confidence: {conf:.1f}%\\\")\\n\",\n",
    "    \"#     print(f\\\"Probabilities: Real={probs[0]:.3f}, Fake={probs[1]:.3f}\\\")\\n\",\n",
    "    \"#     if features:\\n\",\n",
    "    \"#         print(f\\\"\\\\nExtracted Features:\\\")\\n\",\n",
    "    \"#         print(f\\\"  Sentiment: {features.get('textblob_polarity', 0):.3f}\\\")\\n\",\n",
    "    \"#         print(f\\\"  Readability: {features.get('flesch_reading_ease', 0):.1f}\\\")\\n\",\n",
    "    \"#         print(f\\\"  Word count: {features.get('word_count', 0)}\\\")\\n\",\n",
    "    \"# else:\\n\",\n",
    "    \"#     print(\\\"\\\\nSkipping interactive prediction.\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"\\\\nüí° To test your own articles, uncomment the input section above!\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 10. Conclusions <a name=\\\"conclusions\\\"></a>\\n\",\n",
    "    \"\\n\",\n",
    "    \"### üìä Model Performance Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"Our enhanced fake news detection model achieved the following performance:\\n\",\n",
    "    \"\\n\",\n",
    "    \"- **Accuracy**: [Model accuracy from results]\\n\",\n",
    "    \"- **Precision**: [Model precision from results]\\n\",\n",
    "    \"- **Recall**: [Model recall from results]\\n\",\n",
    "    \"- **F1-Score**: [Model F1-score from results]\\n\",\n",
    "    \"\\n\",\n",
    "    \"### üîç Phase 1 Improvements Summary\\n\",\n",
    "    \"\\n\",\n",
    "    \"‚úÖ **Successfully Implemented:**\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Advanced Text Preprocessing**:\\n\",\n",
    "    \"   - Replaced stemming with spaCy lemmatization\\n\",\n",
    "    \"   - Preserved named entities (PERSON, ORG, GPE, DATE, MONEY)\\n\",\n",
    "    \"   - Used POS tagging to filter and analyze tokens\\n\",\n",
    "    \"   - Added toggle for advanced features\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. **Sentiment Analysis**:\\n\",\n",
    "    \"   - TextBlob polarity and subjectivity scores\\n\",\n",
    "    \"   - VADER sentiment analysis (compound, positive, negative, neutral)\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. **Readability Scores**:\\n\",\n",
    "    \"   - Flesch Reading Ease\\n\",\n",
    "    \"   - Flesch-Kincaid Grade Level\\n\",\n",
    "    \"   - Gunning Fog Index\\n\",\n",
    "    \"   - SMOG Index\\n\",\n",
    "    \"   - Automated Readability Index\\n\",\n",
    "    \"   - Coleman-Liau Index\\n\",\n",
    "    \"   - Linsear Write Formula\\n\",\n",
    "    \"   - Dale-Chall Readability Score\\n\",\n",
    "    \"\\n\",\n",
    "    \"4. **Linguistic Features**:\\n\",\n",
    "    \"   - POS tag counts (NOUN, VERB, ADJ, ADV, PROPN)\\n\",\n",
    "    \"   - Named entity counts\\n\",\n",
    "    \"   - Dependency analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"5. **Enhanced Model Training**:\\n\",\n",
    "    \"   - Combined TF-IDF with additional features\\n\",\n",
    "    \"   - Feature scaling for numerical features\\n\",\n",
    "    \"   - Support for multiple model types (Logistic Regression, Random Forest)\\n\",\n",
    "    \"   - Comprehensive feature importance analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"### üöÄ Next Steps for Phase 2\\n\",\n",
    "    \"\\n\",\n",
    "    \"To improve the model further, consider:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Advanced NLP Models**:\\n\",\n",
    "    \"   - Implement BERT or other transformer models\\n\",\n",
    "    \"   - Use pre-trained language models for better text understanding\\n\",\n",
    "    \"   - Experiment with sentence transformers\\n\",\n",
    "    \"\\n\",\n",
    "    \"2. **Feature Engineering**:\\n\",\n",
    "    \"   - Add source credibility features\\n\",\n",
    "    \"   - Include temporal features (publication date analysis)\\n\",\n",
    "    \"   - Extract URL and domain features\\n\",\n",
    "    \"   - Add image analysis features\\n\",\n",
    "    \"\\n\",\n",
    "    \"3. **Ensemble Methods**:\\n\",\n",
    "    \"   - Combine multiple models (voting, stacking)\\n\",\n",
    "    \"   - Use different feature subsets for different models\\n\",\n",
    "    \"   - Implement model selection strategies\\n\",\n",
    "    \"\\n\",\n",
    "    \"4. **Advanced Evaluation**:\\n\",\n",
    "    \"   - Cross-validation with different metrics\\n\",\n",
    "    \"   - Bias analysis and fairness evaluation\\n\",\n",
    "    \"   - Interpretability analysis (SHAP, LIME)\\n\",\n",
    "    \"\\n\",\n",
    "    \"5. **Production Deployment**:\\n\",\n",
    "    \"   - API development with FastAPI\\n\",\n",
    "    \"   - Model versioning and monitoring\\n\",\n",
    "    \"   - Real-time prediction pipeline\\n\",\n",
    "    \"\\n\",\n",
    "    \"### üìÅ Project Structure\\n\",\n",
    "    \"\\n\",\n",
    "    \"```\\n\",\n",
    "    \"fake-news-detection-nlp-ml/\\n\",\n",
    "    \"‚îú‚îÄ‚îÄ data/                 # Dataset files\\n\",\n",
    "    \"‚îú‚îÄ‚îÄ notebooks/           # Jupyter notebooks\\n\",\n",
    "    \"‚îú‚îÄ‚îÄ src/                # Source code modules\\n\",\n",
    "    \"‚îÇ   ‚îú‚îÄ‚îÄ preprocessing.py # Enhanced with Phase 1 features\\n\",\n",
    "    \"‚îÇ   ‚îú‚îÄ‚îÄ model.py        # Enhanced model training\\n\",\n",
    "    \"‚îÇ   ‚îî‚îÄ‚îÄ ...\\n\",\n",
    "    \"‚îú‚îÄ‚îÄ models/             # Trained models\\n\",\n",
    "    \"‚îú‚îÄ‚îÄ outputs/            # Generated plots and reports\\n\",\n",
    "    \"‚îú‚îÄ‚îÄ README.md           # Project documentation\\n\",\n",
    "    \"‚îî‚îÄ‚îÄ requirements.txt    # Updated dependencies\\n\",\n",
    "    \"```\\n\",\n",
    "    \"\\n\",\n",
    "    \"---\\n\",\n",
    "    \"\\n\",\n",
    "    \"**üéâ Congratulations!** You've successfully implemented Phase 1 improvements for the fake news detection system!\\n\",\n",
    "    \"\\n\",\n",
    "    \"This enhanced project demonstrates:\\n\",\n",
    "    \"- ‚úÖ Advanced NLP preprocessing with spaCy\\n\",\n",
    "    \"- ‚úÖ Comprehensive feature engineering\\n\",\n",
    "    \"- ‚úÖ Sentiment and readability analysis\\n\",\n",
    "    \"- ‚úÖ Enhanced model training pipeline\\n\",\n",
    "    \"- ‚úÖ Professional code structure\\n\",\n",
    "    \"- ‚úÖ Production-ready implementation\\n\",\n",
    "    \"\\n\",\n",
    "    \"**üìß Contact**: [Your Name] - [Your Email]\\n\",\n",
    "    \"**üîó GitHub**: [Your GitHub Profile]\\n\",\n",
    "    \"**üìö Portfolio**: [Your Portfolio Website]\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.5\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
